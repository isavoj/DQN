{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HomeWorkA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfrzucCSCJOdjVidWcq1VO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isavoj/DQN/blob/master/HomeWorkA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xty8brR3e1kb",
        "outputId": "fb259d93-c1c1-4744-f01b-f2740cbf6c13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 9.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 10.4 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.8)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/bonlime/pytorch-tools.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-Cocl6tL23o",
        "outputId": "b55889d7-e03d-49a0-d5d6-abba1143110b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/bonlime/pytorch-tools.git@master\n",
            "  Cloning https://github.com/bonlime/pytorch-tools.git (to revision master) to /tmp/pip-req-build-6vt95ghx\n",
            "  Running command git clone -q https://github.com/bonlime/pytorch-tools.git /tmp/pip-req-build-6vt95ghx\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-tools==0.3.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-tools==0.3.0) (0.11.1+cu111)\n",
            "Collecting loguru>=0.4.1\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.49 in /usr/local/lib/python3.7/dist-packages (from pytorch-tools==0.3.0) (4.64.0)\n",
            "Collecting adamp==0.3.0\n",
            "  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->pytorch-tools==0.3.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.7.0->pytorch-tools==0.3.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.7.0->pytorch-tools==0.3.0) (7.1.2)\n",
            "Building wheels for collected packages: pytorch-tools, adamp\n",
            "  Building wheel for pytorch-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-tools: filename=pytorch_tools-0.3.0-py3-none-any.whl size=156583 sha256=ff289ed31fd6526b032173989c2b08bf2c1060cf114597f906b265ee2053a8ad\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4lhlz1n/wheels/68/82/a6/2c97aac95f21ea5471389d44ee000410907930588feeaa0477\n",
            "  Building wheel for adamp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=5998 sha256=267e586e09cac806c3d696a3102992caa050767668849ed45e557d460e8312bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/95/21/ced2d2cb9944e3a72e58fece7958973eed3fd8d0aeb6e2e450\n",
            "Successfully built pytorch-tools adamp\n",
            "Installing collected packages: loguru, adamp, pytorch-tools\n",
            "Successfully installed adamp-0.3.0 loguru-0.6.0 pytorch-tools-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid"
      ],
      "metadata": {
        "id": "j--A3tFpk0vG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root=\"tutorial1\",name= \"Cora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h7Eudnfk30l",
        "outputId": "532e23ad-e495-4e27-c80c-92ae2c243ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## From the Planetoid Cora dataset, extract the number of nodes that are in the \n",
        "* training set, \n",
        "* validation set, \n",
        "* test set"
      ],
      "metadata": {
        "id": "t2nb4Iph7803"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.data)\n",
        "print(\"number of graphs:\\t\\t\",len(dataset))\n",
        "print(\"number of classes:\\t\\t\",dataset.num_classes)\n",
        "print(\"number of node features:\\t\",dataset.num_node_features)\n",
        "print(\"number of edge features:\\t\",dataset.num_edge_features)\n"
      ],
      "metadata": {
        "id": "DyUFd5N8mpxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.data\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Number of training nodes: {data.test_mask.sum()}')\n",
        "print(f'Number of training nodes: {data.val_mask.sum()}')"
      ],
      "metadata": {
        "id": "WEKIWiq6_TZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss briefly the implications for supervised versus semi-supervised learning.\n",
        "\n",
        "Supervised learning - data in our training set are labeled. \n",
        "unsupervised learning - training data is unlabeled. \n",
        "the the usual categories of\n",
        "supervised and unsupervised are not necessarily the most informative or useful\n",
        "when it comes to graphs.\n",
        "Semi supervised - combination of the two above. Pseudo labeling. Machine learning models try to assign similar labels to to neigbouring nodes in a graph,\n",
        "\n",
        "It used the semi-supervised learning method to connect clusters of data based on their similarities. Semi-supervised machine learning uses both labelled and unlabeled data. Semi-supervised algorithm partially trains itself with a small set of labelled data. It then uses this trained part to identify and label the larger unlabelled dataset. \n",
        "\n",
        "Semi-supervised learning helps in labelling data during cluster analysis. Cluster analysis identifies similar data and groups them. This is done using the unsupervised technique. But, semi-supervised machine learning enables the labelling of unlabelled data in the cluster. \n",
        "\n",
        "This method helps overcome the limitations of supervised learning. Since data-labelling is an expensive task, incorporating semi-supervised technique is cost-effective and ensures good results. \n",
        "\n"
      ],
      "metadata": {
        "id": "3X7Hfhf3nZDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (transductive learning)."
      ],
      "metadata": {
        "id": "UD9iE0YIsIW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train both a standard dense network using supervised learning and network using the convolution from equation 1 using semi-supervised learning."
      ],
      "metadata": {
        "id": "ayid7eNYDP1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard network"
      ],
      "metadata": {
        "id": "TClYzx4JFDQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = Linear(dataset.num_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "SM2wZgovFG1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "patience = 4\n",
        "the_last_loss = 1000000\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.x)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "\n",
        "      return test_acc\n",
        "\n",
        "def validation():\n",
        "      model.eval()\n",
        "      out = model(data.x)\n",
        "      loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
        "\n",
        "      return loss\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    loss = train()\n",
        "\n",
        "    # Early stopping\n",
        "    the_current_loss = validation()\n",
        "\n",
        "    if the_current_loss > the_last_loss:\n",
        "        trigger_times += 1\n",
        "        print('trigger times:', trigger_times)\n",
        "\n",
        "        if trigger_times >= patience:\n",
        "            print('Early stopping!\\nStart to test process.')\n",
        "            break\n",
        "\n",
        "    else:\n",
        "        print('trigger times: 0')\n",
        "        trigger_times = 0\n",
        "\n",
        "    the_last_loss = the_current_loss\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "i1QpVuC9JHsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "id": "VCpVGpTBJVx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph convolution\n",
        "Hard to define a kernel in graph convolution do to irregular structure. Need to aggregate node features.\n",
        "\n"
      ],
      "metadata": {
        "id": "OLp76bchUsS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "# Dataset.num_features. num of features per node in a graph. For every node in the graph, get all of the attributes vectores of its connected nodes and apply some aggregation function, like an average. A node might be represented as the avregae of its neighbours.\n",
        "# pass the average vector through a dense neuaral network layer. = multiply it with some matrix and apply an activation function. The output of the ense layer is the new vector representation of the node. \n",
        "# This is done for every node in the graph. ( OBS DOES THIS MEAN THAT WE HAVE NUMBER OF NETWORKS = NBr oF NODES, CREATED WITH GCN CONV)\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "cCTCsbV3Up_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "patience = 4\n",
        "the_last_loss = 10000\n",
        "trigger_times = 0\n",
        "\n",
        "def train():\n",
        "\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "def validation():\n",
        "\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
        "\n",
        "      return loss\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "\n",
        "    loss = train()\n",
        "    # Early stopping\n",
        "    the_current_loss = validation()\n",
        "\n",
        "    if the_current_loss > the_last_loss:\n",
        "        trigger_times += 1\n",
        "        print('trigger times:', trigger_times)\n",
        "\n",
        "        if trigger_times >= patience:\n",
        "            print('Early stopping!\\nStart to test process.')\n",
        "            break\n",
        "\n",
        "    else:\n",
        "        print('trigger times: 0')\n",
        "        trigger_times = 0\n",
        "\n",
        "    the_last_loss = the_current_loss\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "Fd_8HktCYelG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "ca4ktMY3YpsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "POau915qZ-Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1op-CbyLuN4"
      },
      "source": [
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "out = model(data.x, data.edge_index)\n",
        "visualize(out, color=data.y)"
      ],
      "metadata": {
        "id": "IZRwGPihZurk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to outperform the results from (1) using one of the more advanced algorithms\n",
        "such as the graph attention network, from eqn. 3-4. Try to optimize performance, vary-\n",
        "ing the number of heads (for GAT), different pooling layers (?), dropout, etc. (Extensive\n",
        "hyperparameter tuning however is not asked for.)\n",
        "It may not be that easy to improve the results from (1) much (see Cora Benchmark), the\n",
        "main objective here is to show that you have tried."
      ],
      "metadata": {
        "id": "HRd4ppZKvKev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAT"
      ],
      "metadata": {
        "id": "H5lLNE3f2Dpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, heads):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.hid = 8\n",
        "        self.in_head = 8\n",
        "        self.out_head = 1\n",
        "        \n",
        "        \n",
        "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GAT(hidden_channels=8, heads=8)\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  #Compute the loss solely based on the training nodes.\n",
        "      loss.backward()   #Derive gradients.\n",
        "      optimizer.step()  #Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test(mask):\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
        "      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
        "      return acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    loss = train()\n",
        "    val_acc = test(data.val_mask)\n",
        "    test_acc = test(data.test_mask)\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7N0YICGZyRG",
        "outputId": "1d3c702a-212a-49ee-bb68-8ed943490d30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAT(\n",
            "  (conv1): GATConv(1433, 8, heads=8)\n",
            "  (conv2): GATConv(64, 7, heads=1)\n",
            ")\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 001, Loss: 1.9590, Val: 0.3840, Test: 0.4590\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 002, Loss: 1.8685, Val: 0.6040, Test: 0.6510\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 003, Loss: 1.7665, Val: 0.6720, Test: 0.7140\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 004, Loss: 1.6698, Val: 0.7040, Test: 0.7220\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 005, Loss: 1.5698, Val: 0.7080, Test: 0.7250\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 006, Loss: 1.5700, Val: 0.7260, Test: 0.7420\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 007, Loss: 1.3842, Val: 0.7400, Test: 0.7490\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 008, Loss: 1.3283, Val: 0.7460, Test: 0.7500\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 009, Loss: 1.3015, Val: 0.7540, Test: 0.7560\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 010, Loss: 1.2726, Val: 0.7560, Test: 0.7610\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 011, Loss: 1.2053, Val: 0.7600, Test: 0.7700\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 012, Loss: 1.1320, Val: 0.7620, Test: 0.7760\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 013, Loss: 1.0645, Val: 0.7620, Test: 0.7770\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 014, Loss: 1.0928, Val: 0.7620, Test: 0.7780\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 015, Loss: 0.9661, Val: 0.7660, Test: 0.7800\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 016, Loss: 0.9763, Val: 0.7700, Test: 0.7820\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 017, Loss: 0.9085, Val: 0.7640, Test: 0.7780\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 018, Loss: 0.9995, Val: 0.7680, Test: 0.7820\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 019, Loss: 0.8210, Val: 0.7640, Test: 0.7850\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 020, Loss: 0.9040, Val: 0.7660, Test: 0.7850\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 021, Loss: 0.8169, Val: 0.7660, Test: 0.7840\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 022, Loss: 0.7609, Val: 0.7700, Test: 0.7870\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 023, Loss: 0.8116, Val: 0.7640, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 024, Loss: 0.8014, Val: 0.7620, Test: 0.7840\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 025, Loss: 0.7946, Val: 0.7640, Test: 0.7840\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 026, Loss: 0.7440, Val: 0.7640, Test: 0.7820\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 027, Loss: 0.6992, Val: 0.7600, Test: 0.7810\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 028, Loss: 0.7161, Val: 0.7600, Test: 0.7800\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 029, Loss: 0.6938, Val: 0.7640, Test: 0.7770\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 030, Loss: 0.6347, Val: 0.7660, Test: 0.7750\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 031, Loss: 0.6386, Val: 0.7660, Test: 0.7740\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 032, Loss: 0.5763, Val: 0.7700, Test: 0.7790\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 033, Loss: 0.6311, Val: 0.7700, Test: 0.7800\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 034, Loss: 0.5396, Val: 0.7740, Test: 0.7810\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 035, Loss: 0.6457, Val: 0.7760, Test: 0.7830\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 036, Loss: 0.6254, Val: 0.7800, Test: 0.7840\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 037, Loss: 0.5995, Val: 0.7800, Test: 0.7840\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 038, Loss: 0.6312, Val: 0.7780, Test: 0.7830\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 039, Loss: 0.5515, Val: 0.7760, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 040, Loss: 0.6620, Val: 0.7760, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 041, Loss: 0.6621, Val: 0.7740, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 042, Loss: 0.5166, Val: 0.7740, Test: 0.7850\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 043, Loss: 0.6184, Val: 0.7720, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 044, Loss: 0.5687, Val: 0.7720, Test: 0.7850\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 045, Loss: 0.5952, Val: 0.7740, Test: 0.7850\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 046, Loss: 0.5442, Val: 0.7740, Test: 0.7840\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 047, Loss: 0.4976, Val: 0.7720, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 048, Loss: 0.5285, Val: 0.7700, Test: 0.7880\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 049, Loss: 0.4630, Val: 0.7680, Test: 0.7870\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 050, Loss: 0.5134, Val: 0.7720, Test: 0.7880\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 051, Loss: 0.5448, Val: 0.7740, Test: 0.7890\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 052, Loss: 0.6972, Val: 0.7760, Test: 0.7850\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 053, Loss: 0.5200, Val: 0.7740, Test: 0.7870\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 054, Loss: 0.6309, Val: 0.7700, Test: 0.7890\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 055, Loss: 0.4727, Val: 0.7700, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 056, Loss: 0.5588, Val: 0.7720, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 057, Loss: 0.4386, Val: 0.7720, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 058, Loss: 0.5448, Val: 0.7780, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 059, Loss: 0.3852, Val: 0.7800, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 060, Loss: 0.5179, Val: 0.7780, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 061, Loss: 0.4833, Val: 0.7740, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 062, Loss: 0.4806, Val: 0.7740, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 063, Loss: 0.5386, Val: 0.7740, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 064, Loss: 0.4560, Val: 0.7760, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 065, Loss: 0.4887, Val: 0.7760, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 066, Loss: 0.4719, Val: 0.7760, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 067, Loss: 0.5339, Val: 0.7780, Test: 0.7950\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 068, Loss: 0.3724, Val: 0.7780, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 069, Loss: 0.3674, Val: 0.7800, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 070, Loss: 0.3709, Val: 0.7800, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 071, Loss: 0.5221, Val: 0.7800, Test: 0.7890\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 072, Loss: 0.3859, Val: 0.7780, Test: 0.7900\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 073, Loss: 0.4664, Val: 0.7800, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 074, Loss: 0.5203, Val: 0.7800, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 075, Loss: 0.4653, Val: 0.7760, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 076, Loss: 0.4196, Val: 0.7760, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 077, Loss: 0.5081, Val: 0.7740, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 078, Loss: 0.5266, Val: 0.7700, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 079, Loss: 0.4238, Val: 0.7760, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 080, Loss: 0.4110, Val: 0.7760, Test: 0.7950\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 081, Loss: 0.4186, Val: 0.7760, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 082, Loss: 0.4631, Val: 0.7780, Test: 0.7950\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 083, Loss: 0.4131, Val: 0.7780, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 084, Loss: 0.4175, Val: 0.7800, Test: 0.7870\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 085, Loss: 0.3308, Val: 0.7820, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 086, Loss: 0.4648, Val: 0.7800, Test: 0.7890\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 087, Loss: 0.4488, Val: 0.7820, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 088, Loss: 0.4204, Val: 0.7800, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 089, Loss: 0.5198, Val: 0.7800, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 090, Loss: 0.4111, Val: 0.7780, Test: 0.7900\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 091, Loss: 0.4094, Val: 0.7800, Test: 0.7900\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 092, Loss: 0.4630, Val: 0.7780, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 093, Loss: 0.3474, Val: 0.7760, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 094, Loss: 0.4270, Val: 0.7740, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 095, Loss: 0.4277, Val: 0.7740, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 096, Loss: 0.4011, Val: 0.7760, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 097, Loss: 0.3652, Val: 0.7760, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 098, Loss: 0.4423, Val: 0.7760, Test: 0.7900\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 099, Loss: 0.4492, Val: 0.7740, Test: 0.7900\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 100, Loss: 0.4279, Val: 0.7740, Test: 0.7900\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 101, Loss: 0.3793, Val: 0.7760, Test: 0.7880\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 102, Loss: 0.4893, Val: 0.7760, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 103, Loss: 0.3733, Val: 0.7740, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 104, Loss: 0.4214, Val: 0.7780, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 105, Loss: 0.4478, Val: 0.7780, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 106, Loss: 0.3707, Val: 0.7780, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 107, Loss: 0.3865, Val: 0.7800, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 108, Loss: 0.3475, Val: 0.7780, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 109, Loss: 0.3722, Val: 0.7780, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 110, Loss: 0.3755, Val: 0.7780, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 111, Loss: 0.4252, Val: 0.7780, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 112, Loss: 0.4066, Val: 0.7780, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 113, Loss: 0.3145, Val: 0.7760, Test: 0.7950\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 114, Loss: 0.3916, Val: 0.7760, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 115, Loss: 0.3889, Val: 0.7740, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 116, Loss: 0.4450, Val: 0.7740, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 117, Loss: 0.3385, Val: 0.7760, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 118, Loss: 0.4714, Val: 0.7760, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 119, Loss: 0.5452, Val: 0.7760, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 120, Loss: 0.3827, Val: 0.7760, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 121, Loss: 0.4243, Val: 0.7760, Test: 0.7990\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 122, Loss: 0.4625, Val: 0.7760, Test: 0.7990\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 123, Loss: 0.4295, Val: 0.7800, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 124, Loss: 0.4118, Val: 0.7800, Test: 0.7990\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 125, Loss: 0.4165, Val: 0.7780, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 126, Loss: 0.3284, Val: 0.7780, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 127, Loss: 0.4588, Val: 0.7780, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 128, Loss: 0.4563, Val: 0.7800, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 129, Loss: 0.4452, Val: 0.7820, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 130, Loss: 0.4163, Val: 0.7840, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 131, Loss: 0.3671, Val: 0.7820, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 132, Loss: 0.4763, Val: 0.7800, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 133, Loss: 0.3817, Val: 0.7760, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 134, Loss: 0.3915, Val: 0.7760, Test: 0.8020\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 135, Loss: 0.3805, Val: 0.7760, Test: 0.8020\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 136, Loss: 0.3870, Val: 0.7760, Test: 0.8010\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 137, Loss: 0.4706, Val: 0.7760, Test: 0.7990\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 138, Loss: 0.4136, Val: 0.7780, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 139, Loss: 0.4187, Val: 0.7780, Test: 0.7950\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 140, Loss: 0.4617, Val: 0.7760, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 141, Loss: 0.4557, Val: 0.7700, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 142, Loss: 0.4222, Val: 0.7720, Test: 0.7890\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 143, Loss: 0.3674, Val: 0.7720, Test: 0.7860\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 144, Loss: 0.3599, Val: 0.7760, Test: 0.7870\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 145, Loss: 0.3830, Val: 0.7780, Test: 0.7880\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 146, Loss: 0.3232, Val: 0.7780, Test: 0.7870\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 147, Loss: 0.3844, Val: 0.7740, Test: 0.7880\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 148, Loss: 0.3560, Val: 0.7740, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 149, Loss: 0.4609, Val: 0.7740, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 150, Loss: 0.3768, Val: 0.7680, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 151, Loss: 0.3546, Val: 0.7700, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 152, Loss: 0.4212, Val: 0.7720, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 153, Loss: 0.4327, Val: 0.7740, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 154, Loss: 0.3294, Val: 0.7740, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 155, Loss: 0.4153, Val: 0.7740, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 156, Loss: 0.3605, Val: 0.7740, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 157, Loss: 0.4109, Val: 0.7760, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 158, Loss: 0.3866, Val: 0.7760, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 159, Loss: 0.3870, Val: 0.7760, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 160, Loss: 0.3307, Val: 0.7760, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 161, Loss: 0.3443, Val: 0.7780, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 162, Loss: 0.3039, Val: 0.7780, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 163, Loss: 0.3546, Val: 0.7800, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 164, Loss: 0.3089, Val: 0.7800, Test: 0.7900\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 165, Loss: 0.3676, Val: 0.7780, Test: 0.7930\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 166, Loss: 0.3861, Val: 0.7760, Test: 0.7960\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 167, Loss: 0.4628, Val: 0.7740, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 168, Loss: 0.3414, Val: 0.7720, Test: 0.7950\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 169, Loss: 0.3954, Val: 0.7740, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 170, Loss: 0.3024, Val: 0.7740, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 171, Loss: 0.4126, Val: 0.7720, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 172, Loss: 0.4923, Val: 0.7720, Test: 0.7920\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 173, Loss: 0.3365, Val: 0.7780, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 174, Loss: 0.3434, Val: 0.7780, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 175, Loss: 0.3689, Val: 0.7780, Test: 0.7910\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 176, Loss: 0.3191, Val: 0.7760, Test: 0.7940\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 177, Loss: 0.3998, Val: 0.7760, Test: 0.7950\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 178, Loss: 0.4984, Val: 0.7780, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 179, Loss: 0.3995, Val: 0.7740, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 180, Loss: 0.3561, Val: 0.7760, Test: 0.7970\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 181, Loss: 0.2828, Val: 0.7780, Test: 0.7990\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 182, Loss: 0.4579, Val: 0.7760, Test: 0.8010\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 183, Loss: 0.3464, Val: 0.7760, Test: 0.7980\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 184, Loss: 0.3897, Val: 0.7760, Test: 0.8020\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 185, Loss: 0.4536, Val: 0.7760, Test: 0.8020\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 186, Loss: 0.3148, Val: 0.7820, Test: 0.8020\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 187, Loss: 0.3444, Val: 0.7840, Test: 0.7990\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 188, Loss: 0.3556, Val: 0.7840, Test: 0.8010\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 189, Loss: 0.3824, Val: 0.7820, Test: 0.8000\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 190, Loss: 0.3533, Val: 0.7820, Test: 0.8000\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 191, Loss: 0.4017, Val: 0.7820, Test: 0.8020\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 192, Loss: 0.3521, Val: 0.7820, Test: 0.8040\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 193, Loss: 0.3432, Val: 0.7820, Test: 0.8030\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 194, Loss: 0.3646, Val: 0.7840, Test: 0.8030\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 195, Loss: 0.3081, Val: 0.7840, Test: 0.8010\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 196, Loss: 0.3792, Val: 0.7800, Test: 0.8000\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 197, Loss: 0.4039, Val: 0.7820, Test: 0.8000\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 198, Loss: 0.2718, Val: 0.7800, Test: 0.7990\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 199, Loss: 0.3464, Val: 0.7800, Test: 0.8020\n",
            "WORKS 1433\n",
            "WORKS\n",
            "Epoch: 200, Loss: 0.4130, Val: 0.7760, Test: 0.8020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.load('graph_list_2022.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "_sG_F-VHBTVJ",
        "outputId": "dbfd5ff2-136f-4dbe-88bc-48ece4dac57e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d03a4694a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'graph_list_2022.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9jGpydsaPb6h"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}